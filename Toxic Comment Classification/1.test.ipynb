{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# Fotrain = pd.read_csv(\"train.csv\")r example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train = train.sample(frac=1)  #这一步是把顺序打乱，如果想恢复成正常顺序：df = df.sample(frac=1).reset_indedx(drop=True)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values \n",
    "#fillna是吧所有的NA用“CVxTz”进行填充， 之后的.value是用来提取train[\"comment_text\"]填充之后的所有的语句\n",
    "#提取之后的输出是这样的：每一行是一个句子\n",
    "\n",
    "'''\n",
    "[ 'Silly games? \\n\\nIm sorry but what did I DO???????????????????????????????????????????????????????'\n",
    " '\"\\n\\n Surname \\n\\nHow do you pronounce it?  talk \"'\n",
    " \"I also just noticed: he simultaneously went after these same articles on the french Wikipedia, where he has ALSO been banned. See , for example. As I've said before, this editor simply refuses to give up, no matter how many warnings he is given.\"\n",
    " ...,\n",
    " \"Thank you for correcting my typo in Thomas Pooley. I am the world's worst proofreader!\"\n",
    " '(most of which link to a clip of goths beating up emo kids)'\n",
    " '\":You seem to know a lot on this subject!! Have you checked out any of those links?  Smith \\n\\n\"']\n",
    "\n",
    "'''\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values   #这部分就像是training set的label\n",
    "#同样是提取value：提取之后是这样\n",
    "'''\n",
    "[[0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0]\n",
    " ..., \n",
    " [0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0]\n",
    " [0 0 0 0 0 0]]\n",
    "'''\n",
    "\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "train = train.sample(frac=1)  #这一步是把顺序打乱，如果想恢复成正常顺序：df = df.sample(frac=1).reset_indedx(drop=True)\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values   #这部分就像是training set的label\n",
    "#同样是提取value：提取之后是这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100\n",
    "tokenizer = text.Tokenizer(num_words=max_features)  #将文本转化成向量\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))  #先将array转换成list，fit_on_texts里面是要用以训练的文本列表\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)  #之后转换成sequence\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "#将所有的长序列转化成等长序列 \n",
    "#如果提供了参数maxlen，nb_timesteps=maxlen，否则其值为最长序列的长度。其他短于该长度的序列都会在后部填充0以达到该长度\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)   #一开始定义了maxlen=100\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "#可以print一下所有的句子都被向量化并且等长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-63d511dea1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_t' is not defined"
     ]
    }
   ],
   "source": [
    "X_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)    #(20000,128)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)   #全连接\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)   #全连接\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9783Epoch 00001: val_loss improved from inf to 0.04935, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 986s 7ms/step - loss: 0.0664 - acc: 0.9783 - val_loss: 0.0494 - val_acc: 0.9827\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9828Epoch 00002: val_loss improved from 0.04935 to 0.04727, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 1000s 7ms/step - loss: 0.0464 - acc: 0.9828 - val_loss: 0.0473 - val_acc: 0.9833\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "model.load_weights(file_path)\n",
    "\n",
    "y_test = model.predict(X_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('y_test.npy',y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.98250306e-01   4.15136218e-01   9.89175141e-01   5.72846793e-02\n",
      "    8.76540124e-01   1.27772570e-01]\n",
      " [  2.48284778e-04   6.22008329e-08   2.67030155e-05   3.25830342e-07\n",
      "    2.69377360e-05   3.08155245e-06]\n",
      " [  1.79661333e-03   1.92765651e-06   2.85605318e-04   1.07817805e-05\n",
      "    2.37976055e-04   5.32493214e-05]\n",
      " ..., \n",
      " [  3.31642048e-04   4.97843544e-08   2.67797750e-05   1.37724044e-07\n",
      "    3.05294961e-05   1.94226232e-06]\n",
      " [  7.46669422e-04   1.51656039e-07   5.87371032e-05   6.22478296e-07\n",
      "    6.65541666e-05   6.78119113e-06]\n",
      " [  9.87695098e-01   1.01110503e-01   9.23692524e-01   2.58676950e-02\n",
      "    7.22269475e-01   7.98792988e-02]]\n"
     ]
    }
   ],
   "source": [
    "aaa = np.load('y_test.npy')\n",
    "print(np.array(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.98250306e-01,   4.15136218e-01,   9.89175141e-01,\n",
       "          5.72846793e-02,   8.76540124e-01,   1.27772570e-01],\n",
       "       [  2.48284778e-04,   6.22008329e-08,   2.67030155e-05,\n",
       "          3.25830342e-07,   2.69377360e-05,   3.08155245e-06],\n",
       "       [  1.79661333e-03,   1.92765651e-06,   2.85605318e-04,\n",
       "          1.07817805e-05,   2.37976055e-04,   5.32493214e-05],\n",
       "       ..., \n",
       "       [  3.31642048e-04,   4.97843544e-08,   2.67797750e-05,\n",
       "          1.37724044e-07,   3.05294961e-05,   1.94226232e-06],\n",
       "       [  7.46669422e-04,   1.51656039e-07,   5.87371032e-05,\n",
       "          6.22478296e-07,   6.65541666e-05,   6.78119113e-06],\n",
       "       [  9.87695098e-01,   1.01110503e-01,   9.23692524e-01,\n",
       "          2.58676950e-02,   7.22269475e-01,   7.98792988e-02]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "sample_submission[list_classes] = y_test\n",
    "\n",
    "sample_submission.to_csv(\"results2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "results2 = pd.read_csv(\"results2.csv\")\n",
    "\n",
    "print(type(results2['id'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
