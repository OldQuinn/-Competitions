{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RNN方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")   #(159571, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data\n",
    "#train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_data\n",
    "#test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data = train_data.sample(frac=1)  #这一步是把顺序打乱，如果想恢复成正常顺序：df = df.sample(frac=1).reset_indedx(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "记住下面的几种pandas操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_data.loc[0:3,:]\n",
    "#train_data[\"comment_text\"]\n",
    "#train_data.loc[0,:][\"comment_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除comment是none的数据，但是这组数据里面没有事none的，可以比较处理前后的shape来判断是否有none  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n,_ = train_data.shape\n",
    "for i in range(n):\n",
    "    if train_data.loc[i,:][\"comment_text\"] == None:\n",
    "        train_data.drop(i,axis=0,inplace=True)   \n",
    "#如果想要把none的地方用别的东西替换，可以用train_data[\"comment_text\"].fillna(\"*****\")  *****是用来替换的的东西     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取train所有的评论数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments_train = train_data[\"comment_text\"].values  \n",
    "#print(comments_train.shape)   #(159571,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取所有的labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "labels = train_data[classes].values\n",
    "#print(labels.shape)     #(159571, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取test的所有评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_test,_ = test_data.shape\n",
    "for i in range(n_test):\n",
    "    if test_data.loc[i,:][\"comment_text\"] == None:\n",
    "        test_data.drop(i,axis=0,inplace=True)\n",
    "        \n",
    "comments_test = test_data[\"comment_text\"].values  \n",
    "\n",
    "#print(comments_test.shape)      #(153164,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "进行one-hot编码，将文本转化成向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_len = 20000   #编码的单词的数量的上限\n",
    "tokenizer = text.Tokenizer(num_words=dic_len)  #分词器\n",
    "tokenizer.fit_on_texts(list(comments_train))  #先将array转换成list，fit_on_texts里面是要用以训练的文本列表，之后把每个单词编码\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(comments_train)  #之后句子就会变成单词编码的序列\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(comments_test)\n",
    "\n",
    "#最后生成的序列可以print一下list_tokenized_train或者list_tokenized_train[0]看看\n",
    "#使用例子：http://blog.csdn.net/zzulp/article/details/76146947 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "将所有的长序列转化成等长序列 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#如果提供了参数maxlen，nb_timesteps=maxlen，否则其值为最长序列的长度。其他短于该长度的序列都会在后部填充0以达到该长度\n",
    "maxlen=2000\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)   \n",
    "X_test  = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)   \n",
    "#train和test的maxlen必须相同，所以必须要提前定义maxlen\n",
    "#X_train = sequence.pad_sequences(list_tokenized_train)   \n",
    "#X_test  = sequence.pad_sequences(list_tokenized_test)\n",
    "#可以print一下所有的句子都被向量化并且等长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(X_train[0])\n",
    "#len(X_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "构建RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation,BatchNormalization\n",
    "#from keras.models import \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 2000, 64)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 2000, 200)         132000    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 2000, 200)         240800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2000, 200)         0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2000, 50)          10050     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2000, 50)          0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2000, 6)           306       \n",
      "=================================================================\n",
      "Total params: 1,663,156\n",
      "Trainable params: 1,663,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#下面的这个模型的问题在于，Sequential()只能讲不同的layer叠加，但是在中间的时候无法对数据进行处理，而在RNN之后我们要选取最后一个cell的输出\n",
    "RNN_model=Sequential()\n",
    "RNN_model.add(Embedding(input_dim = dic_len, output_dim = 64, input_length = maxlen))\n",
    "#input_dim是字典里面包含的单词数量，output_dim是embedding扩展的维度，input_length：当输入序列的长度固定时，该值为其长度 \n",
    "RNN_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "RNN_model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "#RNN_model.add(GlobalMaxPool1D())\n",
    "RNN_model.add(Dropout(0.3))\n",
    "RNN_model.add(Dense(50, activation=\"relu\"))\n",
    "RNN_model.add(Dropout(0.1))\n",
    "RNN_model.add(Dense(6, activation=\"sigmoid\"))\n",
    "\n",
    "RNN_model.compile(loss='binary_crossentropy',metrics=['acc'],optimizer=Adam(0.005))\n",
    "RNN_model.summary()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    embed_size = 64\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(dic_len, embed_size)(inp)    #(20000,128)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    #x = x[:, -1, :]\n",
    "    #x = np.reshape((embed_size, embed_size))\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)   #全连接\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)   #全连接\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_6 (Embedding)      (None, 2000, 64)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 2000, 200)         132000    \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 2000, 200)         240800    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 1,663,156\n",
      "Trainable params: 1,663,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "RNN_model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNN_model.fit(X_train, labels, batch_size=64, epochs=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_6 to have 3 dimensions, but got array with shape (159571, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-d5d68c471328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1594\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1431\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1432\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_6 to have 3 dimensions, but got array with shape (159571, 6)"
     ]
    }
   ],
   "source": [
    "#RNN_model.fit(X_train, labels, batch_size=64, epochs=2, validation_split=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
