{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_as_stacker_final(pinjie_train,pinjie_test,old_train,test):\n",
    "    #havent del other 5 labels,will do  when I feel it nessisary.\n",
    "    import lightgbm as lgb\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "    submission_test = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for class_name in class_names:\n",
    "\n",
    "        train_sparse_matrix, valid_sparse_matrix, y_train__, y_valid__ = train_test_split(pinjie_train, old_train[class_name], test_size=0.05, random_state=144)\n",
    "        test_sparse_matrix = pinjie_test\n",
    "#         print(y_train__[class_name])\n",
    "#         print(y_valid__)\n",
    "        d_train = lgb.Dataset(train_sparse_matrix, label=y_train__)\n",
    "#         d_train_new = lgb.Dataset(new_train_sparse_matrix, label=train_y[class_name])\n",
    "        d_valid = lgb.Dataset(valid_sparse_matrix, label=y_valid__)\n",
    "\n",
    "\n",
    "        watchlist = [d_train, d_valid]\n",
    "        params = {'learning_rate': 0.2,\n",
    "                  'application': 'binary',\n",
    "                  'num_leaves': 31,\n",
    "                  'verbosity': -1,\n",
    "                  'metric': 'auc',\n",
    "                  'data_random_seed': 2,\n",
    "                  'bagging_fraction': 0.8,\n",
    "                  'feature_fraction': 0.6,\n",
    "                  'nthread': 4,\n",
    "                  'lambda_l1': 1,\n",
    "                  'lambda_l2': 1}\n",
    "        rounds_lookup = {'toxic': 140,\n",
    "                     'severe_toxic': 50,\n",
    "                     'obscene': 80,\n",
    "                     'threat': 80,\n",
    "                     'insult': 70,\n",
    "                     'identity_hate': 80}\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train,\n",
    "                          num_boost_round=rounds_lookup[class_name],\n",
    "                          valid_sets=watchlist,\n",
    "                          verbose_eval=10)\n",
    "\n",
    "\n",
    "        submission_test[class_name] = model.predict(test_sparse_matrix)\n",
    "#         submission_valid[class_name] = model.predict(valid_sparse_matrix)\n",
    "    submission_test.to_csv('stack_with_lgbm_final.csv', index=False)\n",
    "    \n",
    "    return submission_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb(old_train,test,train_x,train_y,valid_x,valid_x2,valid_y):\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import lightgbm as lgb\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "    class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#     train = pd.read_csv('train.csv').fillna(' ')\n",
    "#     test = pd.read_csv('test.csv').fillna(' ')\n",
    "    print('Loaded')\n",
    "\n",
    "#     train=train[:1000]\n",
    "#     test=test[:1000]\n",
    "    train_text = old_train['comment_text']\n",
    "    test_text = test['comment_text']\n",
    "\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=50000)\n",
    "    word_vectorizer.fit(all_text)\n",
    "    print('Word TFIDF 1/3')\n",
    "    train_word_features = word_vectorizer.transform(train_text)\n",
    "    train_word_features_new = word_vectorizer.transform(train_x)\n",
    "    valid_word_features_new = word_vectorizer.transform(valid_x)\n",
    "\n",
    "\n",
    "    print('Word TFIDF 2/3')\n",
    "    test_word_features = word_vectorizer.transform(test_text)\n",
    "    print('Word TFIDF 3/3')\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char',\n",
    "        stop_words='english',\n",
    "        ngram_range=(2, 6),\n",
    "        max_features=50000)\n",
    "    char_vectorizer.fit(all_text)\n",
    "    print('Char TFIDF 1/3')\n",
    "    train_char_features = char_vectorizer.transform(train_text)\n",
    "    new_train_char_features = char_vectorizer.transform(train_x)\n",
    "    new_valid_char_features = char_vectorizer.transform(valid_x)\n",
    "    print('Char TFIDF 2/3')\n",
    "    test_char_features = char_vectorizer.transform(test_text)\n",
    "    print('Char TFIDF 3/3')\n",
    "\n",
    "    train_features = hstack([train_char_features, train_word_features])\n",
    "    new_train_features=hstack([new_train_char_features,train_word_features_new])\n",
    "    new_valid_features=hstack([new_valid_char_features,valid_word_features_new])\n",
    "    print('HStack 1/2')\n",
    "    test_features = hstack([test_char_features, test_word_features])\n",
    "    print('HStack 2/2')\n",
    "\n",
    "    submission_test = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    submission_valid= pd.DataFrame.from_dict({'id': valid_x2['id']})\n",
    "    old_train2=old_train.drop('comment_text', axis=1, inplace=False)\n",
    "    del test\n",
    "    del train_text\n",
    "    del test_text\n",
    "    del all_text\n",
    "    del train_char_features\n",
    "    del test_char_features\n",
    "    del train_word_features\n",
    "    del test_word_features\n",
    "    del train_word_features_new\n",
    "    del valid_word_features_new\n",
    "    del new_valid_char_features\n",
    "    del new_train_char_features\n",
    "    \n",
    "    gc.collect()\n",
    "#     for i, class_name in enumerate(class_names):\n",
    "#         print(i,class_name)\n",
    "    for class_name in class_names:\n",
    "#         print(class_name,'tset1')\n",
    "        train_target = old_train2[class_name]\n",
    "        model = LogisticRegression(solver='sag')\n",
    "        sfm = SelectFromModel(model, threshold='1.25*mean')\n",
    "#         print(train_features.shape,'tset12')\n",
    "        train_sparse_matrix = sfm.fit_transform(train_features, train_target)\n",
    "#         print(train_sparse_matrix.shape,'tset123')\n",
    "        \n",
    "        \n",
    "        \n",
    "#         train_sparse_matrix, valid_sparse_matrix, y_train, y_valid = train_test_split(train_sparse_matrix, train_target, test_size=0.05, random_state=144)\n",
    "        test_sparse_matrix = sfm.transform(test_features)\n",
    "        valid_sparse_matrix = sfm.transform(new_valid_features)\n",
    "        new_train_sparse_matrix = sfm.transform(new_train_features)\n",
    "#         print('tset1234')\n",
    "\n",
    "        \n",
    "#         d_train = lgb.Dataset(train_sparse_matrix, label=y_train)\n",
    "        d_train_new = lgb.Dataset(new_train_sparse_matrix, label=train_y[class_name])\n",
    "        d_valid = lgb.Dataset(valid_sparse_matrix, label=valid_y[class_name])\n",
    "        \n",
    "#         print ('12345')\n",
    "        \n",
    "        watchlist = [d_train_new, d_valid]\n",
    "        params = {'learning_rate': 0.2,\n",
    "                  'application': 'binary',\n",
    "                  'num_leaves': 31,\n",
    "                  'verbosity': -1,\n",
    "                  'metric': 'auc',\n",
    "                  'data_random_seed': 2,\n",
    "                  'bagging_fraction': 0.8,\n",
    "                  'feature_fraction': 0.6,\n",
    "                  'nthread': 4,\n",
    "                  'lambda_l1': 1,\n",
    "                  'lambda_l2': 1}\n",
    "        rounds_lookup = {'toxic': 140,\n",
    "                     'severe_toxic': 50,\n",
    "                     'obscene': 80,\n",
    "                     'threat': 80,\n",
    "                     'insult': 70,\n",
    "                     'identity_hate': 80}\n",
    "        model = lgb.train(params,\n",
    "                          train_set=d_train_new,\n",
    "                          num_boost_round=rounds_lookup[class_name],\n",
    "                          valid_sets=watchlist,\n",
    "                          verbose_eval=10)\n",
    "        \n",
    "#         submission[class_name] = model.predict(train_sparse_matrix)\n",
    "#         print('123456')\n",
    "        submission_test[class_name] = model.predict(test_sparse_matrix)\n",
    "        submission_valid[class_name] = model.predict(valid_sparse_matrix)\n",
    "#         print(submission_test)\n",
    "    return submission_test,submission_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lgb_get_train_test(old_train,test):\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import lightgbm as lgb\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "\n",
    "    class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "#     train = pd.read_csv('train.csv').fillna(' ')\n",
    "#     test = pd.read_csv('test.csv').fillna(' ')\n",
    "    print('Loaded')\n",
    "    train_ids=old_train.id\n",
    "    test_ids=test.id\n",
    "    check_id=[test_ids==train_ids]\n",
    "    [i for i in check_id[0] if i == False]\n",
    "\n",
    "    \n",
    "#     train=train[:1000]\n",
    "#     test=test[:1000]\n",
    "    train_text = old_train['comment_text']\n",
    "    test_text = test['comment_text']\n",
    "\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=50000)\n",
    "    word_vectorizer.fit(all_text)\n",
    "    print('Word TFIDF 1/3')\n",
    "    train_word_features = word_vectorizer.transform(train_text)\n",
    "#     train_word_features_new = word_vectorizer.transform(train_x)\n",
    "#     valid_word_features_new = word_vectorizer.transform(valid_x)\n",
    "\n",
    "\n",
    "    print('Word TFIDF 2/3')\n",
    "    test_word_features = word_vectorizer.transform(test_text)\n",
    "    print('Word TFIDF 3/3')\n",
    "\n",
    "    char_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='char',\n",
    "        stop_words='english',\n",
    "        ngram_range=(2, 6),\n",
    "        max_features=50000)\n",
    "    char_vectorizer.fit(all_text)\n",
    "    print('Char TFIDF 1/3')\n",
    "    train_char_features = char_vectorizer.transform(train_text)\n",
    "#     new_train_char_features = char_vectorizer.transform(train_x)\n",
    "#     new_valid_char_features = char_vectorizer.transform(valid_x)\n",
    "    print('Char TFIDF 2/3')\n",
    "    test_char_features = char_vectorizer.transform(test_text)\n",
    "    print('Char TFIDF 3/3')\n",
    "\n",
    "    train_features = hstack([train_char_features, train_word_features])\n",
    "#     new_train_features=hstack([new_train_char_features,train_word_features_new])\n",
    "#     new_valid_features=hstack([new_valid_char_features,valid_word_features_new])\n",
    "    print('HStack 1/2')\n",
    "    test_features = hstack([test_char_features, test_word_features])\n",
    "    print('HStack 2/2')\n",
    "\n",
    "    submission_test = pd.DataFrame.from_dict({'id': test['id']})\n",
    "#     submission_valid= pd.DataFrame.from_dict({'id': valid_x2['id']})\n",
    "    old_train=old_train.drop('comment_text', axis=1, inplace=False)\n",
    "    del test\n",
    "    del train_text\n",
    "    del test_text\n",
    "    del all_text\n",
    "    del train_char_features\n",
    "    del test_char_features\n",
    "    del train_word_features\n",
    "    del test_word_features\n",
    "#     del train_word_features_new\n",
    "\n",
    "    gc.collect()\n",
    "#     for i, class_name in enumerate(class_names):\n",
    "#         print(i,class_name)\n",
    "    for class_name in class_names:\n",
    "#         print(class_name,'tset1')\n",
    "        train_target = old_train[class_name]\n",
    "        model = LogisticRegression(solver='sag')\n",
    "        sfm = SelectFromModel(model, threshold='1.15*mean')\n",
    "#         print(train_features.shape,'tset12')\n",
    "        train_sparse_matrix = sfm.fit_transform(train_features, train_target)\n",
    "#         print(train_sparse_matrix.shape,'tset123')\n",
    "        \n",
    "        \n",
    "        \n",
    "#         train_sparse_matrix, valid_sparse_matrix, y_train, y_valid = train_test_split(train_sparse_matrix, train_target, test_size=0.05, random_state=144)\n",
    "        test_sparse_matrix = sfm.transform(test_features)\n",
    "\n",
    "\n",
    "#         d_train = lgb.Dataset(train_sparse_matrix, label=train_target)\n",
    "\n",
    "\n",
    "    return train_sparse_matrix,test_sparse_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_(old_train,valid_x2,test,train_x,train_y,valid_x,valid_y):\n",
    "    import pandas as pd, numpy as np\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#     train = pd.read_csv('train.csv')\n",
    "#     test = pd.read_csv('test.csv')\n",
    "    subm = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "    COMMENT = 'comment_text'\n",
    "    old_train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    import re, string\n",
    "    re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "    n = old_train.shape[0]\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "                   min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                   smooth_idf=1, sublinear_tf=1 )\n",
    "    trn_term_doc = vec.fit_transform(old_train[COMMENT])\n",
    "   \n",
    "    train_term_doc = vec.transform(train_x)\n",
    "    valid_term_doc = vec.transform(valid_x)\n",
    "\n",
    "    test_term_doc = vec.transform(test[COMMENT])\n",
    "\n",
    "\n",
    "\n",
    "    def pr(y_i, y):\n",
    "        p = x[y==y_i].sum(0)\n",
    "        return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "    x = trn_term_doc\n",
    "    train_x=train_term_doc\n",
    "    test_x = test_term_doc\n",
    "    valid_x=valid_term_doc\n",
    "\n",
    "    submission_test = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    submission_valid= pd.DataFrame.from_dict({'id': valid_x2['id']})\n",
    "\n",
    "    def get_mdl(y):\n",
    "        y = y.values\n",
    "        r = np.log(pr(1,y) / pr(0,y))\n",
    "        m = LogisticRegression(C=4, dual=True)\n",
    "        x_nb = x.multiply(r)\n",
    "        return m.fit(x_nb, y), r\n",
    "\n",
    "    preds_test = np.zeros((len(test), len(label_cols)))\n",
    "    preds_valid= np.zeros((len(x_valid), len(label_cols)))\n",
    "\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fit', j)\n",
    "        m,r = get_mdl(old_train[j])\n",
    "        \n",
    "        preds_test[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "        preds_valid[:,i] = m.predict_proba(valid_x.multiply(r))[:,1]\n",
    "    df1=submission_valid\n",
    "    df1_test=submission_test\n",
    "    \n",
    "    \n",
    "    df2=pd.DataFrame(preds_valid, columns = label_cols)\n",
    "    df2_test=pd.DataFrame(preds_test, columns = label_cols)\n",
    "    df1.index = range(len(df1))\n",
    "    df2.index = range(len(df2))\n",
    "    df1_test.index = range(len(df1_test))\n",
    "    df2_test.index = range(len(df2_test))\n",
    "    \n",
    "    submission_test=pd.concat([df1_test,df2_test],axis=1)\n",
    "    submission_valid=pd.concat([df1,df2],axis=1)\n",
    "\n",
    "    return submission_test,submission_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_(old_train,valid_x2,test,train_x,train_y,valid_x,valid_y,top_word=2000,maxlen=50,word_vec_len=20):\n",
    "    import pandas as pd\n",
    "\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras.models import Sequential\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Dense, Embedding, Input\n",
    "    from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "    from keras.preprocessing import text, sequence\n",
    "#     train=pd.read_csv('train.csv')\n",
    "#     test = pd.read_csv(\"test.csv\")\n",
    "    train['comment_text'] = old_train['comment_text'].str.replace('[^a-zA-Z]',' ').str.lower()\n",
    "    test['comment_text'] = test['comment_text'].str.replace('[^a-zA-Z]',' ').str.lower()\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "    # train_split = train['comment_text'].str.split()\n",
    "    # test_split = test['comment_text'].str.split()\n",
    "#     seqence_train = train_x\n",
    "#     seqence_valid = valid_x\n",
    "    \n",
    "    seqence_train=train[\"comment_text\"].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    seqence_test = test[\"comment_text\"].values\n",
    "    list_classes=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "    # y=train[list_classes].values\n",
    "\n",
    "\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#     top_word=20000\n",
    "#     maxlen=100\n",
    "#     word_vec_len=50\n",
    "\n",
    "\n",
    "    # 1. token and 词序号\n",
    "\n",
    "    tokenizer=Tokenizer(top_word)\n",
    "    tokenizer.fit_on_texts(list(seqence_train))\n",
    "#     tokenizer.fit_on_texts(list(seqence_valid))\n",
    " \n",
    "    sequence_train2=tokenizer.texts_to_sequences(train_x)\n",
    "    sequence_valid2=tokenizer.texts_to_sequences(valid_x)\n",
    "    # tokenizer.fit_on_texts(list(seqence_test))\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(seqence_test)\n",
    "\n",
    "#     y = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "\n",
    "\n",
    "    # 2. pad_sequence\n",
    "    pad_train=pad_sequences(sequence_train2,maxlen)\n",
    "    pad_valid=pad_sequences(sequence_valid2,maxlen)\n",
    "    pad_test = pad_sequences(list_tokenized_test, maxlen)\n",
    "\n",
    "    # # 3. embedding\n",
    "    # model=Sequential()\n",
    "    # model.add(Embedding(mask_zero=True,input_dim=top_word,input_length=max_review_len,output_dim=word_vec_len))\n",
    "    # model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "    # word2vec_train=model.predict(paded_sequence_train)\n",
    "    # # word2vec_test=model.predict(X_te)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, LSTM, Bidirectional, GlobalMaxPool1D, BatchNormalization,Embedding\n",
    "\n",
    "\n",
    "    import logging\n",
    "    from keras.callbacks import Callback\n",
    "\n",
    "    import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.layers import Dense, Input, LSTM, GRU, Embedding, Dropout, Activation, BatchNormalization\n",
    "    from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "    from keras.models import Model\n",
    "    from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "\n",
    "\n",
    "    from keras.layers import Activation,BatchNormalization\n",
    "    from keras.optimizers import Adam\n",
    "    embedding_layer=Embedding(input_dim=top_word,input_length=maxlen,output_dim=word_vec_len)\n",
    "\n",
    "    bi_rnn=Sequential()\n",
    "    bi_rnn.add(embedding_layer)\n",
    "    bi_rnn.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1),input_shape=(maxlen,word_vec_len)))\n",
    "    bi_rnn.add(GlobalMaxPool1D())\n",
    "    bi_rnn.add(BatchNormalization())\n",
    "    bi_rnn.add( Dense(50))\n",
    "    bi_rnn.add(Activation('relu'))\n",
    "    bi_rnn.add(Dropout(0.1))\n",
    "    bi_rnn.add( Dense(6))\n",
    "    # bi_rnn.add(BatchNormalization())\n",
    "    bi_rnn.add(Activation('sigmoid'))\n",
    "    # bi_rnn.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "    bi_rnn.summary()\n",
    "\n",
    "\n",
    "\n",
    "    bi_rnn.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    # early_stopping_monitor = EarlyStopping(patience=2)\n",
    "#     y = old_train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "\n",
    "    # lr = callbacks.LearningRateScheduler(schedule)\n",
    "    filepath = \"/Users/bruce/All ucl/1applied ml/kaggle/toxic/final/output/keras_nei_{val_acc:.4f}.hdf5\"\n",
    "    Checkpoint=ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=False)\n",
    "\n",
    "    # monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto'\n",
    "    early = EarlyStopping(monitor='val_acc',patience=0,verbose=1,mode='auto')\n",
    "    bi_rnn.fit(x=pad_train,y=train_y,validation_data=(pad_valid,valid_y),epochs=3,batch_size=64,callbacks=[early,Checkpoint])\n",
    "\n",
    "    #model.fit(x=train_padded,y=y,validation_split=.1,epochs=3,batch_size=64)\n",
    "    test_pred = bi_rnn.predict(x=pad_test)\n",
    "    valid_pred = bi_rnn.predict(x=pad_valid)\n",
    "    submission_test = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    submission_valid= pd.DataFrame.from_dict({'id': valid_x2['id']})\n",
    "    \n",
    "    \n",
    "    df1=submission_valid\n",
    "    df1_test=submission_test\n",
    "    df2=pd.DataFrame(valid_pred, columns = label_cols)\n",
    "    df2_test=pd.DataFrame(test_pred, columns = label_cols)\n",
    "    \n",
    "    \n",
    "    df1.index = range(len(df1))\n",
    "    df2.index = range(len(df2))\n",
    "    df1_test.index = range(len(df1_test))\n",
    "    df2_test.index = range(len(df2_test))\n",
    "\n",
    "    submission_test=pd.concat([df1_test,df2_test],axis=1)\n",
    "    submission_valid=pd.concat([df1,df2],axis=1)\n",
    "\n",
    "    return submission_test,submission_valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "old_train = pd.read_csv('train.csv').fillna(' ')\n",
    "# old_train=old_train.head(1000)\n",
    "test = pd.read_csv('test.csv').fillna(' ')\n",
    "train=old_train\n",
    "# test=tes\n",
    "\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "i,P1,P2,T1,T2=0,[],[],[],[]\n",
    "kf=KFold(n_splits=2)\n",
    "\n",
    "for train_index, valid_index in kf.split(train):\n",
    "#     print(valid_index)\n",
    "    train_n,valid_n=train.loc[train_index],train.loc[valid_index]\n",
    "    x_train=train_n['comment_text']\n",
    "    x_valid=valid_n['comment_text']\n",
    "    x_valid2=valid_n\n",
    "    y_train=train_n[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "    y_valid=valid_n[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "    \n",
    "    \n",
    "    T_1,P_1=lstm_(valid_x2=x_valid2,test=test,old_train=old_train,train_x=x_train,train_y=y_train,valid_x=x_valid,valid_y=y_valid)\n",
    "    print(i,'m1')\n",
    "\n",
    "    T_2,P_2=lr_(valid_x2=x_valid2,test=test,old_train=old_train,train_x=x_train,train_y=y_train,valid_x=x_valid,valid_y=y_valid)\n",
    "  \n",
    "    print(i,'m2')\n",
    "\n",
    "#     T_3,P_3=lgb(valid_x2=x_valid2,test=test,old_train=old_train,train_x=x_train,train_y=y_train,valid_x=x_valid,valid_y=y_valid)\n",
    "#     print(i,'m3')\n",
    "\n",
    "    i+=1\n",
    "    P1.append(P_1)\n",
    "    P2.append(P_2)\n",
    "#     P.append(P_3)\n",
    "    \n",
    "    T1.append(T_1)\n",
    "    T2.append(T_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def check_id(a,b):\n",
    "    test_id=a.id\n",
    "    test_id2=b.id\n",
    "    check_id=[T1[0].id==T1[1].id]\n",
    "    return [i for i in check_id[0] if i == False]\n",
    "\n",
    "\n",
    "P1_final = pd.concat([i for i in P1], axis=0)\n",
    "P2_final = pd.concat([i for i in P2], axis=0)  \n",
    "\n",
    "\n",
    "check_id(P1_final,P2_final)\n",
    "check_id(P1_final,old_train)\n",
    "pp1=P1_final[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "pp2=P2_final[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "print(pp1.shape)\n",
    "final_P=np.concatenate((pp1,pp2),axis=1)\n",
    "# print(final_p.shape)\n",
    "train_sparse_matrix,test_sparse_matrix=lgb_get_train_test(old_train,test)\n",
    "\n",
    "\n",
    "def pinjie_train(train_sparse_matrix,final_P):\n",
    "    from scipy.sparse import coo_matrix,hstack\n",
    "    final_P=coo_matrix(final_P)\n",
    "    final_trainnn=hstack([final_P,train_sparse_matrix])\n",
    "#     final_trainnn=np.concatenate((train_sparse_matrix,final_P),axis=1)\n",
    "    return final_trainnn\n",
    "\n",
    "\n",
    "    \n",
    "#     T.append(T_3)  \n",
    "\n",
    "# [i for i in P1]\n",
    "test_id=T1[0].id\n",
    "test_id2=T1[1].id\n",
    "check_id=[T1[0].id==T1[1].id]\n",
    "[i for i in check_id[0] if i == False]\n",
    "\n",
    "\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "a_=np.zeros_like(T1[0][label_cols])\n",
    "print(a_.shape)\n",
    "for i in T1:    \n",
    "    i=i.drop('id',axis=1,inplace=False)\n",
    "    a_+=i.values\n",
    "    T1_final=a_/len(T1)\n",
    "    \n",
    "b_=np.zeros_like(T2[0][label_cols])  \n",
    "print(b_.shape)\n",
    "for i in T2:       \n",
    "    i=i.drop('id',axis=1,inplace=False)\n",
    "    b_+=i.values\n",
    "    T2_final=b_/len(T2)\n",
    "    \n",
    "all_Ts=np.concatenate((T1_final,T2_final),axis=1)\n",
    "print(all_Ts.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pinjie_test(embeded_old_test_x,all_Ts):\n",
    "    from scipy.sparse import coo_matrix,hstack\n",
    "    all_Ts=coo_matrix(all_Ts)\n",
    "    bbb_=hstack([all_Ts,embeded_old_test_x])\n",
    "#     all_P.drop('id',axis=1,inplace=True)\n",
    "#     print(all_Ts.shape)\n",
    "#     print(embeded_old_test_x.shape)\n",
    "#     bbb_=np.concatenate((all_Ts,embeded_old_train_x),axis=1)\n",
    "# #         print(all_Ts.shape)\n",
    "#     print(bbb_.shape)\n",
    "    return bbb_\n",
    "\n",
    "\n",
    "final_train=pinjie_train(train_sparse_matrix,final_P)\n",
    "\n",
    "final_test=pinjie_test(test_sparse_matrix,all_Ts)\n",
    "\n",
    "stacked_test=lgb_as_stacker_final(old_train=old_train,pinjie_test=final_test,pinjie_train=final_train,test=test)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold,train_test_split\n",
    "\n",
    "\n",
    "def get_data(old_train,test):\n",
    "    from sklearn.model_selection import KFold,train_test_split\n",
    "    for train_index, valid_index in kf.split(old_train)\n",
    "        train,valid=old_train[i],old_train[j]\n",
    "        x_train=train['comment_text']\n",
    "        x_valid=valid['comment_text']\n",
    "        y_train=train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "        y_valid=valid[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]]\n",
    "    return x_train,x_valid,y_train,y_valid,test\n",
    "\n",
    "def stacker(cv,x,y,test):\n",
    "    kf=KFold(n_splits=cv)\n",
    "    for train_index, valid_index in kf.split(x)\n",
    "        x_train,x_valid=train_text[i],train_text[j]\n",
    "        for class_name in class_names:\n",
    "            print(class_name)\n",
    "            train_target = train[class_name]\n",
    "            y_train,y_valid=train_target[train_index],train_target[valid_index]\n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for cv[1]:\n",
    "        train_x,train_y,valid_x,valid_y=train_test_split(x,y)\n",
    "\n",
    "\n",
    "        Model=fit! train_x with M[1] \n",
    "        P_1(1st/3)=Model.pred(valid_x)\n",
    "        T_1_1=Model.pred(test)\n",
    "\n",
    "        Model=fit! train_x with M[2] \n",
    "        P_2(1st/3)=Model.pred(valid_x)\n",
    "        T_1_3=Model.pred(test)\n",
    "\n",
    "\n",
    "        Model=fit! train_x with M[3] \n",
    "        P_3(1st/3)=Model.pred(valid_x)\n",
    "        T_1_3=Model.pred(test)\n",
    "        \n",
    "        T_1=np.mean(T_1_1, T_1_2, T_1_3)\n",
    "\n",
    "            \n",
    "    for cv[2]\n",
    "        train_x_2,train_y_2,valid_x_2,valid_y_2=train_test_split(x,y)\n",
    "        \n",
    "        \n",
    "        Model=fit! train_x_2 with M[1] \n",
    "        P_1(2nd/3)=Model.pred(valid_x_2)\n",
    "\n",
    "        Model=fit! train_x with M[2] \n",
    "        P_2(2nd/3)=Model.pred(valid_x_2)\n",
    "\n",
    "        Model=fit! train_x with M[3] \n",
    "        P_3(2nd/3)=Model.pred(valid_x_2)\n",
    "        \n",
    "        T_2=np.mean(T_2_1, T_2_2, T_2_3)\n",
    "        \n",
    "    for cv[3]\n",
    "        train_x_3,train_y_3,valid_x_3,valid_y_3=train_test_split(x,y)\n",
    "    \n",
    "    \n",
    "        Model=fit! train_x_3 with M[1] \n",
    "        P_1(3nd/3)=Model.pred(valid_x_3)\n",
    "\n",
    "        Model=fit! train_x with M[2] \n",
    "        P_2(3nd/3)=Model.pred(valid_x_3)\n",
    "\n",
    "        Model=fit! train_x with M[3] \n",
    "        P_3(3nd/3)=Model.pred(valid_x_3)\n",
    "        \n",
    "        T_3=np.mean()\n",
    "        \n",
    "        \n",
    "    stack and get: p_1,P_2,P_3\n",
    "    \n",
    "    new_x=old_x+p1+p2+p3\n",
    "    new_test=old_test+t1+t2+t3\n",
    "    \n",
    "    fit new_x with staker model M4\n",
    "    pred new_test \n",
    "    \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
